{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 143.43059921264648,
  "kg_co2_emissions": 0.006885003173846981,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.699542236328125,
        "ap": 0.6353018621846855,
        "ap_weighted": 0.6353018621846855,
        "f1": 0.6949564650767359,
        "f1_weighted": 0.6949457184227896,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.699542236328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.729736328125,
            "ap": 0.6628761662565795,
            "ap_weighted": 0.6628761662565795,
            "f1": 0.729125544526029,
            "f1_weighted": 0.729120049043776
          },
          {
            "accuracy": 0.680206298828125,
            "ap": 0.6143341560396277,
            "ap_weighted": 0.6143341560396277,
            "f1": 0.6711665678620677,
            "f1_weighted": 0.6711432738746712
          },
          {
            "accuracy": 0.751007080078125,
            "ap": 0.6790328693376418,
            "ap_weighted": 0.6790328693376418,
            "f1": 0.7491265795803503,
            "f1_weighted": 0.7491172997047885
          },
          {
            "accuracy": 0.65887451171875,
            "ap": 0.6080099535641545,
            "ap_weighted": 0.6080099535641545,
            "f1": 0.6575602666501315,
            "f1_weighted": 0.6575693304092254
          },
          {
            "accuracy": 0.677886962890625,
            "ap": 0.6122561657254239,
            "ap_weighted": 0.6122561657254239,
            "f1": 0.6678033667651013,
            "f1_weighted": 0.6677786390814865
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}