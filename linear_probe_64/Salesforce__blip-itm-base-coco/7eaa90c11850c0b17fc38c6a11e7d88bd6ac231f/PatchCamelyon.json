{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 294.0931010246277,
  "kg_co2_emissions": 0.015149345492558669,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.724139404296875,
        "ap": 0.6703977791555897,
        "ap_weighted": 0.6703977791555897,
        "f1": 0.7211016536778102,
        "f1_weighted": 0.7211029922809953,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.724139404296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.737030029296875,
            "ap": 0.6842727208051593,
            "ap_weighted": 0.6842727208051593,
            "f1": 0.7355650123542707,
            "f1_weighted": 0.735573421635614
          },
          {
            "accuracy": 0.64996337890625,
            "ap": 0.5918605513964283,
            "ap_weighted": 0.5918605513964283,
            "f1": 0.6407845749172341,
            "f1_weighted": 0.6407600420314291
          },
          {
            "accuracy": 0.747528076171875,
            "ap": 0.693799720537339,
            "ap_weighted": 0.693799720537339,
            "f1": 0.7464858681575459,
            "f1_weighted": 0.7464928129039528
          },
          {
            "accuracy": 0.74505615234375,
            "ap": 0.6996501912654863,
            "ap_weighted": 0.6996501912654863,
            "f1": 0.7418139641150066,
            "f1_weighted": 0.7418263253991031
          },
          {
            "accuracy": 0.741119384765625,
            "ap": 0.6824057117735359,
            "ap_weighted": 0.6824057117735359,
            "f1": 0.7408588488449939,
            "f1_weighted": 0.7408623594348773
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}