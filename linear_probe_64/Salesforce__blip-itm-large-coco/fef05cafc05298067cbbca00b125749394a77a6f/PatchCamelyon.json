{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 537.406938791275,
  "kg_co2_emissions": 0.04002167117419207,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.724481201171875,
        "ap": 0.6609397864119693,
        "ap_weighted": 0.6609397864119693,
        "f1": 0.7227095978204131,
        "f1_weighted": 0.7227045557213848,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.724481201171875,
        "scores_per_experiment": [
          {
            "accuracy": 0.744415283203125,
            "ap": 0.6830204627290433,
            "ap_weighted": 0.6830204627290433,
            "f1": 0.744386618815721,
            "f1_weighted": 0.7443877753039736
          },
          {
            "accuracy": 0.675567626953125,
            "ap": 0.6114060198810725,
            "ap_weighted": 0.6114060198810725,
            "f1": 0.6683041504557621,
            "f1_weighted": 0.668283179395551
          },
          {
            "accuracy": 0.740081787109375,
            "ap": 0.6749124100927375,
            "ap_weighted": 0.6749124100927375,
            "f1": 0.7399407863501912,
            "f1_weighted": 0.739938199180298
          },
          {
            "accuracy": 0.72467041015625,
            "ap": 0.6665704372892047,
            "ap_weighted": 0.6665704372892047,
            "f1": 0.7243006685401292,
            "f1_weighted": 0.7243049821923173
          },
          {
            "accuracy": 0.7376708984375,
            "ap": 0.6687896020677886,
            "ap_weighted": 0.6687896020677886,
            "f1": 0.736615764940262,
            "f1_weighted": 0.736608642534784
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}