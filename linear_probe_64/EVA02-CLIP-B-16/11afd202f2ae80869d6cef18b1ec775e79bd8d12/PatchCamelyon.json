{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 126.31393218040466,
  "kg_co2_emissions": 0.004134323964699337,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.727105712890625,
        "ap": 0.6575336765937756,
        "ap_weighted": 0.6575336765937756,
        "f1": 0.7210116556363116,
        "f1_weighted": 0.720995315181385,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.727105712890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.750335693359375,
            "ap": 0.679720130898301,
            "ap_weighted": 0.679720130898301,
            "f1": 0.749017220230646,
            "f1_weighted": 0.7490094481785188
          },
          {
            "accuracy": 0.648101806640625,
            "ap": 0.5894486017377197,
            "ap_weighted": 0.5894486017377197,
            "f1": 0.6325993982181457,
            "f1_weighted": 0.6325671543141322
          },
          {
            "accuracy": 0.749755859375,
            "ap": 0.6740437795794417,
            "ap_weighted": 0.6740437795794417,
            "f1": 0.7452960702648935,
            "f1_weighted": 0.7452816705768074
          },
          {
            "accuracy": 0.776214599609375,
            "ap": 0.7056346569653608,
            "ap_weighted": 0.7056346569653608,
            "f1": 0.7753118792548421,
            "f1_weighted": 0.7753057944763331
          },
          {
            "accuracy": 0.71112060546875,
            "ap": 0.6388212137880548,
            "ap_weighted": 0.6388212137880548,
            "f1": 0.7028337102130311,
            "f1_weighted": 0.7028125083611341
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}