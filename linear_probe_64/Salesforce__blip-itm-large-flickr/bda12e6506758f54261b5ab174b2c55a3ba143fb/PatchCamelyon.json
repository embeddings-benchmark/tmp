{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 531.77152967453,
  "kg_co2_emissions": 0.0391446288443722,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.714306640625,
        "ap": 0.6523460784295615,
        "ap_weighted": 0.6523460784295615,
        "f1": 0.712872554801336,
        "f1_weighted": 0.7128685453517021,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.714306640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.737030029296875,
            "ap": 0.6759100854866991,
            "ap_weighted": 0.6759100854866991,
            "f1": 0.736989453962128,
            "f1_weighted": 0.7369908496738884
          },
          {
            "accuracy": 0.67169189453125,
            "ap": 0.6087315488290054,
            "ap_weighted": 0.6087315488290054,
            "f1": 0.6651852527251093,
            "f1_weighted": 0.6651653111784179
          },
          {
            "accuracy": 0.731536865234375,
            "ap": 0.6673666031051539,
            "ap_weighted": 0.6673666031051539,
            "f1": 0.7314553218191315,
            "f1_weighted": 0.7314533225059732
          },
          {
            "accuracy": 0.72412109375,
            "ap": 0.6656977337912174,
            "ap_weighted": 0.6656977337912174,
            "f1": 0.7238064465135405,
            "f1_weighted": 0.7238104293899514
          },
          {
            "accuracy": 0.7071533203125,
            "ap": 0.6440244209357313,
            "ap_weighted": 0.6440244209357313,
            "f1": 0.7069262989867706,
            "f1_weighted": 0.7069228140102792
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}