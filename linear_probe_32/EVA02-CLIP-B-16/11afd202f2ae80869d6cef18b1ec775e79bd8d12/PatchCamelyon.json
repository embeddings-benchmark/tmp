{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 173.90968203544617,
  "kg_co2_emissions": 0.005475037480248074,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.685302734375,
        "ap": 0.6228304970571813,
        "ap_weighted": 0.6228304970571813,
        "f1": 0.6769784385001116,
        "f1_weighted": 0.6769581246152874,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.685302734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.7327880859375,
            "ap": 0.6606491230789824,
            "ap_weighted": 0.6606491230789824,
            "f1": 0.7295197772948192,
            "f1_weighted": 0.7295070742628933
          },
          {
            "accuracy": 0.6282958984375,
            "ap": 0.5756550456397362,
            "ap_weighted": 0.5756550456397362,
            "f1": 0.6118446690116364,
            "f1_weighted": 0.6118105277113159
          },
          {
            "accuracy": 0.7525634765625,
            "ap": 0.6833538366517448,
            "ap_weighted": 0.6833538366517448,
            "f1": 0.7517567407463884,
            "f1_weighted": 0.7517506945464818
          },
          {
            "accuracy": 0.66900634765625,
            "ap": 0.6083366782581591,
            "ap_weighted": 0.6083366782581591,
            "f1": 0.6660094094596922,
            "f1_weighted": 0.6659958923415377
          },
          {
            "accuracy": 0.64385986328125,
            "ap": 0.586157801657284,
            "ap_weighted": 0.586157801657284,
            "f1": 0.6257615959880221,
            "f1_weighted": 0.6257264342142079
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}