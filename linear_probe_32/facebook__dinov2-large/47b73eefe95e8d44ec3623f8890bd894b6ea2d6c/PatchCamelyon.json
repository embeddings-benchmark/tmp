{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 199.482727766037,
  "kg_co2_emissions": 0.014368983125843024,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.779705810546875,
        "ap": 0.7158726898872979,
        "ap_weighted": 0.7158726898872979,
        "f1": 0.7795429592137828,
        "f1_weighted": 0.7795410937147856,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.779705810546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.793487548828125,
            "ap": 0.7320977338134288,
            "ap_weighted": 0.7320977338134288,
            "f1": 0.793485263733583,
            "f1_weighted": 0.7934849702352014
          },
          {
            "accuracy": 0.77783203125,
            "ap": 0.7126203366898289,
            "ap_weighted": 0.7126203366898289,
            "f1": 0.7777223180840047,
            "f1_weighted": 0.7777202082154279
          },
          {
            "accuracy": 0.80389404296875,
            "ap": 0.7437603052850358,
            "ap_weighted": 0.7437603052850358,
            "f1": 0.8038934285729133,
            "f1_weighted": 0.80389328027047
          },
          {
            "accuracy": 0.748443603515625,
            "ap": 0.6800119456285297,
            "ap_weighted": 0.6800119456285297,
            "f1": 0.7477798369712192,
            "f1_weighted": 0.7477743088738833
          },
          {
            "accuracy": 0.774871826171875,
            "ap": 0.7108731280196665,
            "ap_weighted": 0.7108731280196665,
            "f1": 0.7748339487071942,
            "f1_weighted": 0.774832700978946
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}