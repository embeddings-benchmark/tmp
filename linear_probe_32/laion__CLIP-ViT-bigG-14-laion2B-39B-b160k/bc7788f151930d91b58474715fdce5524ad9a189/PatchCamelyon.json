{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 384.47845220565796,
  "kg_co2_emissions": 0.022548118008066356,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.754449462890625,
        "ap": 0.6996446221209921,
        "ap_weighted": 0.6996446221209921,
        "f1": 0.7532347517241972,
        "f1_weighted": 0.7532396487640635,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.754449462890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.81585693359375,
            "ap": 0.7602644447512378,
            "ap_weighted": 0.7602644447512378,
            "f1": 0.8158228138562674,
            "f1_weighted": 0.815823884879417
          },
          {
            "accuracy": 0.750946044921875,
            "ap": 0.6899930133161241,
            "ap_weighted": 0.6899930133161241,
            "f1": 0.7508996906842067,
            "f1_weighted": 0.7509011424947825
          },
          {
            "accuracy": 0.75689697265625,
            "ap": 0.6988438482682845,
            "ap_weighted": 0.6988438482682845,
            "f1": 0.7566355363763471,
            "f1_weighted": 0.756638944298059
          },
          {
            "accuracy": 0.711395263671875,
            "ap": 0.6515807015364974,
            "ap_weighted": 0.6515807015364974,
            "f1": 0.7113272428460282,
            "f1_weighted": 0.7113291360698092
          },
          {
            "accuracy": 0.737152099609375,
            "ap": 0.6975411027328168,
            "ap_weighted": 0.6975411027328168,
            "f1": 0.7314884748581365,
            "f1_weighted": 0.7315051360782494
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}