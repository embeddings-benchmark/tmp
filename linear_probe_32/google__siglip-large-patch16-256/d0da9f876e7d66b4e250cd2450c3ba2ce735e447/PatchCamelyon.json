{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 211.7172622680664,
  "kg_co2_emissions": 0.01492836473723057,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.777197265625,
        "ap": 0.7203636297766185,
        "ap_weighted": 0.7203636297766185,
        "f1": 0.7765872928119816,
        "f1_weighted": 0.7765898342688585,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.777197265625,
        "scores_per_experiment": [
          {
            "accuracy": 0.80535888671875,
            "ap": 0.7543288790353971,
            "ap_weighted": 0.7543288790353971,
            "f1": 0.8050080165161853,
            "f1_weighted": 0.8050115504606715
          },
          {
            "accuracy": 0.785186767578125,
            "ap": 0.7165283019672188,
            "ap_weighted": 0.7165283019672188,
            "f1": 0.7846778817866351,
            "f1_weighted": 0.7846734094695724
          },
          {
            "accuracy": 0.767303466796875,
            "ap": 0.7060509414210023,
            "ap_weighted": 0.7060509414210023,
            "f1": 0.7672884758343781,
            "f1_weighted": 0.7672892738323817
          },
          {
            "accuracy": 0.7547607421875,
            "ap": 0.6977736849212061,
            "ap_weighted": 0.6977736849212061,
            "f1": 0.7543572824761816,
            "f1_weighted": 0.7543615358165118
          },
          {
            "accuracy": 0.77337646484375,
            "ap": 0.7271363415382682,
            "ap_weighted": 0.7271363415382682,
            "f1": 0.7716048074465272,
            "f1_weighted": 0.7716134017651555
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}