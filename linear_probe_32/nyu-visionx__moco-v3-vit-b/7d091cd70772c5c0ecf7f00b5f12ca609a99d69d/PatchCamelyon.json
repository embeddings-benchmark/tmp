{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 1409.8852138519287,
  "kg_co2_emissions": 0.032141553011842794,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.76072998046875,
        "ap": 0.7035971939528689,
        "ap_weighted": 0.7035971939528689,
        "f1": 0.759550972174422,
        "f1_weighted": 0.7595527819079319,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.76072998046875,
        "scores_per_experiment": [
          {
            "accuracy": 0.779571533203125,
            "ap": 0.7340905528272414,
            "ap_weighted": 0.7340905528272414,
            "f1": 0.7778996610243608,
            "f1_weighted": 0.7779078939510238
          },
          {
            "accuracy": 0.733001708984375,
            "ap": 0.6613644085589079,
            "ap_weighted": 0.6613644085589079,
            "f1": 0.7301526295804284,
            "f1_weighted": 0.7301407830964203
          },
          {
            "accuracy": 0.765472412109375,
            "ap": 0.7119000468396793,
            "ap_weighted": 0.7119000468396793,
            "f1": 0.76472815883904,
            "f1_weighted": 0.7647338124178704
          },
          {
            "accuracy": 0.768310546875,
            "ap": 0.7093450896408768,
            "ap_weighted": 0.7093450896408768,
            "f1": 0.7681910445895586,
            "f1_weighted": 0.7681932932884781
          },
          {
            "accuracy": 0.757293701171875,
            "ap": 0.7012858718976395,
            "ap_weighted": 0.7012858718976395,
            "f1": 0.7567833668387225,
            "f1_weighted": 0.7567881267858672
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}