{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 191.9119040966034,
  "kg_co2_emissions": 0.013923786991362257,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.7138916015625,
        "ap": 0.6451229548784495,
        "ap_weighted": 0.6451229548784495,
        "f1": 0.7096855542218435,
        "f1_weighted": 0.7096728076029359,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7138916015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.71600341796875,
            "ap": 0.6435408701905532,
            "ap_weighted": 0.6435408701905532,
            "f1": 0.7091444503962532,
            "f1_weighted": 0.7091253674181102
          },
          {
            "accuracy": 0.7159423828125,
            "ap": 0.644275443891158,
            "ap_weighted": 0.644275443891158,
            "f1": 0.710312728422134,
            "f1_weighted": 0.7102954746652458
          },
          {
            "accuracy": 0.7203369140625,
            "ap": 0.6581402826738666,
            "ap_weighted": 0.6581402826738666,
            "f1": 0.7203329092253113,
            "f1_weighted": 0.7203324570662739
          },
          {
            "accuracy": 0.691131591796875,
            "ap": 0.6283516727918839,
            "ap_weighted": 0.6283516727918839,
            "f1": 0.6902328708162424,
            "f1_weighted": 0.6902257421512403
          },
          {
            "accuracy": 0.726043701171875,
            "ap": 0.6513065048447857,
            "ap_weighted": 0.6513065048447857,
            "f1": 0.7184048122492761,
            "f1_weighted": 0.718384996713809
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}