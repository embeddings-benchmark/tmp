{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 117.3650586605072,
  "kg_co2_emissions": 0.005478093068806627,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.637127685546875,
        "ap": 0.5842150608570534,
        "ap_weighted": 0.5842150608570534,
        "f1": 0.611691261376621,
        "f1_weighted": 0.6116538087806781,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.637127685546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.610687255859375,
            "ap": 0.5628805004831117,
            "ap_weighted": 0.5628805004831117,
            "f1": 0.572487766486031,
            "f1_weighted": 0.5724331679305203
          },
          {
            "accuracy": 0.587493896484375,
            "ap": 0.5479958749736095,
            "ap_weighted": 0.5479958749736095,
            "f1": 0.5192098456632266,
            "f1_weighted": 0.5191324323737703
          },
          {
            "accuracy": 0.707000732421875,
            "ap": 0.6390726278581227,
            "ap_weighted": 0.6390726278581227,
            "f1": 0.7040807832973591,
            "f1_weighted": 0.7040682243763935
          },
          {
            "accuracy": 0.63629150390625,
            "ap": 0.5845677014808762,
            "ap_weighted": 0.5845677014808762,
            "f1": 0.6350227418605372,
            "f1_weighted": 0.6350135479326697
          },
          {
            "accuracy": 0.6441650390625,
            "ap": 0.5865585994895476,
            "ap_weighted": 0.5865585994895476,
            "f1": 0.6276551695759511,
            "f1_weighted": 0.6276216712900364
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}