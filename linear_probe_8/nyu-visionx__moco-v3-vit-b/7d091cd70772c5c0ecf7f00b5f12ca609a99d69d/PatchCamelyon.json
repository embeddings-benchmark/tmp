{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 1478.1842832565308,
  "kg_co2_emissions": 0.03324836807585414,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.6920654296875,
        "ap": 0.6307488764052454,
        "ap_weighted": 0.6307488764052454,
        "f1": 0.6860819860970779,
        "f1_weighted": 0.6860650520028416,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6920654296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.6566162109375,
            "ap": 0.5968549177880504,
            "ap_weighted": 0.5968549177880504,
            "f1": 0.6481053873857332,
            "f1_weighted": 0.6480820060023493
          },
          {
            "accuracy": 0.63751220703125,
            "ap": 0.5828482459918036,
            "ap_weighted": 0.5828482459918036,
            "f1": 0.6275848172911593,
            "f1_weighted": 0.6275588390750171
          },
          {
            "accuracy": 0.76263427734375,
            "ap": 0.6958378282280797,
            "ap_weighted": 0.6958378282280797,
            "f1": 0.7623771040090688,
            "f1_weighted": 0.7623737640956314
          },
          {
            "accuracy": 0.760589599609375,
            "ap": 0.6916116538175665,
            "ap_weighted": 0.6916116538175665,
            "f1": 0.7599396655393901,
            "f1_weighted": 0.7599343288373491
          },
          {
            "accuracy": 0.642974853515625,
            "ap": 0.5865917362007265,
            "ap_weighted": 0.5865917362007265,
            "f1": 0.6324029562600386,
            "f1_weighted": 0.6323763220038612
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}