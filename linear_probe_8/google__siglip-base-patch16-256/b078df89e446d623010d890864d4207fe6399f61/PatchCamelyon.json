{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 128.69660878181458,
  "kg_co2_emissions": 0.006688151333792704,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.66148681640625,
        "ap": 0.6045323293699311,
        "ap_weighted": 0.6045323293699311,
        "f1": 0.6576000688717784,
        "f1_weighted": 0.6575876382919414,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.66148681640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.5860595703125,
            "ap": 0.5487079818213484,
            "ap_weighted": 0.5487079818213484,
            "f1": 0.5786685660885815,
            "f1_weighted": 0.5786447241394721
          },
          {
            "accuracy": 0.6380615234375,
            "ap": 0.5871640341662366,
            "ap_weighted": 0.5871640341662366,
            "f1": 0.6379216695963981,
            "f1_weighted": 0.6379186292955045
          },
          {
            "accuracy": 0.71112060546875,
            "ap": 0.6420888541477099,
            "ap_weighted": 0.6420888541477099,
            "f1": 0.7077597982338745,
            "f1_weighted": 0.707746408563616
          },
          {
            "accuracy": 0.65216064453125,
            "ap": 0.5988112471440312,
            "ap_weighted": 0.5988112471440312,
            "f1": 0.6521533554890179,
            "f1_weighted": 0.6521526751784095
          },
          {
            "accuracy": 0.72003173828125,
            "ap": 0.6458895295703293,
            "ap_weighted": 0.6458895295703293,
            "f1": 0.7114969549510198,
            "f1_weighted": 0.7114757542827048
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}