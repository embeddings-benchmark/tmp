{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 446.5481593608856,
  "kg_co2_emissions": 0.025540556351379567,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.72159423828125,
        "ap": 0.6503342598930966,
        "ap_weighted": 0.6503342598930966,
        "f1": 0.714898573716685,
        "f1_weighted": 0.714881199635325,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.72159423828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.7012939453125,
            "ap": 0.6314939747971335,
            "ap_weighted": 0.6314939747971335,
            "f1": 0.6940914619011499,
            "f1_weighted": 0.6940714072536243
          },
          {
            "accuracy": 0.723602294921875,
            "ap": 0.65148623431357,
            "ap_weighted": 0.65148623431357,
            "f1": 0.7190457836011233,
            "f1_weighted": 0.7190304969587819
          },
          {
            "accuracy": 0.74249267578125,
            "ap": 0.6737160501434507,
            "ap_weighted": 0.6737160501434507,
            "f1": 0.7416129513618195,
            "f1_weighted": 0.7416065098650244
          },
          {
            "accuracy": 0.748626708984375,
            "ap": 0.6738627054068175,
            "ap_weighted": 0.6738627054068175,
            "f1": 0.7448454933392998,
            "f1_weighted": 0.7448322225899815
          },
          {
            "accuracy": 0.69195556640625,
            "ap": 0.6211123348045111,
            "ap_weighted": 0.6211123348045111,
            "f1": 0.6748971783800325,
            "f1_weighted": 0.6748653615092135
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}