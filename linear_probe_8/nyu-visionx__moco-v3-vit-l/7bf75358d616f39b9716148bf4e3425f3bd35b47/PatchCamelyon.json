{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 3822.52703666687,
  "kg_co2_emissions": 0.08646474293522913,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.69898681640625,
        "ap": 0.6376266673569869,
        "ap_weighted": 0.6376266673569869,
        "f1": 0.6953085649588895,
        "f1_weighted": 0.6952969657984622,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.69898681640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.694915771484375,
            "ap": 0.6340417658401125,
            "ap_weighted": 0.6340417658401125,
            "f1": 0.6948344241669935,
            "f1_weighted": 0.6948322954521461
          },
          {
            "accuracy": 0.6409912109375,
            "ap": 0.585750271487475,
            "ap_weighted": 0.585750271487475,
            "f1": 0.6333383805223951,
            "f1_weighted": 0.6333157485777792
          },
          {
            "accuracy": 0.76043701171875,
            "ap": 0.6966730003360123,
            "ap_weighted": 0.6966730003360123,
            "f1": 0.7604187607885874,
            "f1_weighted": 0.7604178673864116
          },
          {
            "accuracy": 0.754547119140625,
            "ap": 0.684119206244841,
            "ap_weighted": 0.684119206244841,
            "f1": 0.7534040163908406,
            "f1_weighted": 0.7533968431777088
          },
          {
            "accuracy": 0.64404296875,
            "ap": 0.5875490928764939,
            "ap_weighted": 0.5875490928764939,
            "f1": 0.6345472429256304,
            "f1_weighted": 0.6345220743982656
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}