{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 1270.4482052326202,
  "kg_co2_emissions": 0.11904431741322812,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.655194091796875,
        "ap": 0.5978860674123532,
        "ap_weighted": 0.5978860674123532,
        "f1": 0.6489630207032019,
        "f1_weighted": 0.648947895945007,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.655194091796875,
        "scores_per_experiment": [
          {
            "accuracy": 0.660614013671875,
            "ap": 0.5996197490166397,
            "ap_weighted": 0.5996197490166397,
            "f1": 0.6514225225803831,
            "f1_weighted": 0.6513983389917192
          },
          {
            "accuracy": 0.624755859375,
            "ap": 0.5776852944618148,
            "ap_weighted": 0.5776852944618148,
            "f1": 0.6247551198879571,
            "f1_weighted": 0.6247548948266831
          },
          {
            "accuracy": 0.679412841796875,
            "ap": 0.6176660184742597,
            "ap_weighted": 0.6176660184742597,
            "f1": 0.6777512081327931,
            "f1_weighted": 0.6777413216596537
          },
          {
            "accuracy": 0.65582275390625,
            "ap": 0.6004680826998451,
            "ap_weighted": 0.6004680826998451,
            "f1": 0.6554372848510366,
            "f1_weighted": 0.6554323609762438
          },
          {
            "accuracy": 0.655364990234375,
            "ap": 0.5939911924092064,
            "ap_weighted": 0.5939911924092064,
            "f1": 0.6354489680638395,
            "f1_weighted": 0.635412563270735
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}