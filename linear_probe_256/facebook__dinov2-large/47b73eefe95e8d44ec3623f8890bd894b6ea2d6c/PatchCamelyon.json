{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 205.70109391212463,
  "kg_co2_emissions": 0.01478256497118795,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.81751708984375,
        "ap": 0.7669959545869428,
        "ap_weighted": 0.7669959545869428,
        "f1": 0.817109279804028,
        "f1_weighted": 0.8171121354233115,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.81751708984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.823089599609375,
            "ap": 0.7733200785295116,
            "ap_weighted": 0.7733200785295116,
            "f1": 0.8228845276095321,
            "f1_weighted": 0.8228871025045974
          },
          {
            "accuracy": 0.809906005859375,
            "ap": 0.7609378989690686,
            "ap_weighted": 0.7609378989690686,
            "f1": 0.8094680601364068,
            "f1_weighted": 0.8094719628990558
          },
          {
            "accuracy": 0.828399658203125,
            "ap": 0.7703245907159258,
            "ap_weighted": 0.7703245907159258,
            "f1": 0.8283909815531201,
            "f1_weighted": 0.828390460209343
          },
          {
            "accuracy": 0.79974365234375,
            "ap": 0.7568988312382375,
            "ap_weighted": 0.7568988312382375,
            "f1": 0.798411251621308,
            "f1_weighted": 0.7984182537272068
          },
          {
            "accuracy": 0.826446533203125,
            "ap": 0.7734983734819707,
            "ap_weighted": 0.7734983734819707,
            "f1": 0.826391578099773,
            "f1_weighted": 0.8263928977763544
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}