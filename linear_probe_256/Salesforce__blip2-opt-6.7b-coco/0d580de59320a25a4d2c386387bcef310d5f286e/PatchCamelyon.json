{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 1359.1625196933746,
  "kg_co2_emissions": 0.12640126811619867,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.805487060546875,
        "ap": 0.7578482742527456,
        "ap_weighted": 0.7578482742527456,
        "f1": 0.8048198717773676,
        "f1_weighted": 0.804824634917526,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.805487060546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.801971435546875,
            "ap": 0.7519692824080729,
            "ap_weighted": 0.7519692824080729,
            "f1": 0.8014832284769013,
            "f1_weighted": 0.8014874345685812
          },
          {
            "accuracy": 0.817352294921875,
            "ap": 0.7715941484173938,
            "ap_weighted": 0.7715941484173938,
            "f1": 0.8167976402865932,
            "f1_weighted": 0.8168019470890857
          },
          {
            "accuracy": 0.79998779296875,
            "ap": 0.7505853744155782,
            "ap_weighted": 0.7505853744155782,
            "f1": 0.79940779564558,
            "f1_weighted": 0.7994124040238572
          },
          {
            "accuracy": 0.796173095703125,
            "ap": 0.7523528532600171,
            "ap_weighted": 0.7523528532600171,
            "f1": 0.7948464947318514,
            "f1_weighted": 0.7948535430861581
          },
          {
            "accuracy": 0.81195068359375,
            "ap": 0.7627397127626662,
            "ap_weighted": 0.7627397127626662,
            "f1": 0.8115641997459118,
            "f1_weighted": 0.811567845819948
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}