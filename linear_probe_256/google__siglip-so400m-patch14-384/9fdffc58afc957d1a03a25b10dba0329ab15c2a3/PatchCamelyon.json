{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 791.6739521026611,
  "kg_co2_emissions": 0.06308982326932445,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.82882080078125,
        "ap": 0.782907749187047,
        "ap_weighted": 0.782907749187047,
        "f1": 0.8284834991188692,
        "f1_weighted": 0.8284867416334144,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.82882080078125,
        "scores_per_experiment": [
          {
            "accuracy": 0.824951171875,
            "ap": 0.7779255599985263,
            "ap_weighted": 0.7779255599985263,
            "f1": 0.8246171620065226,
            "f1_weighted": 0.824620432033207
          },
          {
            "accuracy": 0.844024658203125,
            "ap": 0.8029901518882141,
            "ap_weighted": 0.8029901518882141,
            "f1": 0.8436486185820854,
            "f1_weighted": 0.8436518945962077
          },
          {
            "accuracy": 0.821044921875,
            "ap": 0.7738289390900995,
            "ap_weighted": 0.7738289390900995,
            "f1": 0.8206630889296327,
            "f1_weighted": 0.8206666244198675
          },
          {
            "accuracy": 0.829833984375,
            "ap": 0.7830328096771996,
            "ap_weighted": 0.7830328096771996,
            "f1": 0.8295557194080063,
            "f1_weighted": 0.8295586617868114
          },
          {
            "accuracy": 0.824249267578125,
            "ap": 0.7767612852811954,
            "ap_weighted": 0.7767612852811954,
            "f1": 0.8239329066680985,
            "f1_weighted": 0.8239360953309786
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}