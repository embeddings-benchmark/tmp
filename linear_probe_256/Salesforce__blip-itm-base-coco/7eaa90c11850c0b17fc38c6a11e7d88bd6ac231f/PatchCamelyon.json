{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 304.259747505188,
  "kg_co2_emissions": 0.015869219042772532,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.758526611328125,
        "ap": 0.7073453010349005,
        "ap_weighted": 0.7073453010349005,
        "f1": 0.7570316486328077,
        "f1_weighted": 0.7570391694070372,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.758526611328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.747650146484375,
            "ap": 0.7008665772421587,
            "ap_weighted": 0.7008665772421587,
            "f1": 0.744924945662711,
            "f1_weighted": 0.7449362101479498
          },
          {
            "accuracy": 0.7725830078125,
            "ap": 0.7156363699278834,
            "ap_weighted": 0.7156363699278834,
            "f1": 0.7723329387406064,
            "f1_weighted": 0.7723361624671317
          },
          {
            "accuracy": 0.751007080078125,
            "ap": 0.6956047469562052,
            "ap_weighted": 0.6956047469562052,
            "f1": 0.7503279480913275,
            "f1_weighted": 0.7503335114896981
          },
          {
            "accuracy": 0.76092529296875,
            "ap": 0.7176697563570198,
            "ap_weighted": 0.7176697563570198,
            "f1": 0.7579482978882144,
            "f1_weighted": 0.7579597667740503
          },
          {
            "accuracy": 0.760467529296875,
            "ap": 0.7069490546912354,
            "ap_weighted": 0.7069490546912354,
            "f1": 0.7596241127811791,
            "f1_weighted": 0.7596301961563567
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}