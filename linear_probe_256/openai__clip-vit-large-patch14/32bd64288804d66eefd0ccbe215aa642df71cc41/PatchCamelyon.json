{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 203.81060576438904,
  "kg_co2_emissions": 0.014433898275855568,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.806396484375,
        "ap": 0.757452571954046,
        "ap_weighted": 0.757452571954046,
        "f1": 0.8058511506102872,
        "f1_weighted": 0.805855399476024,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.806396484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.803497314453125,
            "ap": 0.7532080156406805,
            "ap_weighted": 0.7532080156406805,
            "f1": 0.8030594979394869,
            "f1_weighted": 0.8030634652088663
          },
          {
            "accuracy": 0.821075439453125,
            "ap": 0.7734182208485637,
            "ap_weighted": 0.7734182208485637,
            "f1": 0.8207229419252826,
            "f1_weighted": 0.8207263383226604
          },
          {
            "accuracy": 0.790008544921875,
            "ap": 0.7403221720000877,
            "ap_weighted": 0.7403221720000877,
            "f1": 0.7892506806733861,
            "f1_weighted": 0.7892560802151056
          },
          {
            "accuracy": 0.7987060546875,
            "ap": 0.7529810432519151,
            "ap_weighted": 0.7529810432519151,
            "f1": 0.7976989516101738,
            "f1_weighted": 0.7977050499852094
          },
          {
            "accuracy": 0.818695068359375,
            "ap": 0.7673334080289834,
            "ap_weighted": 0.7673334080289834,
            "f1": 0.818523680903107,
            "f1_weighted": 0.8185260636482785
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}