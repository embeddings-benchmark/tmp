{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 1617.862323999405,
  "kg_co2_emissions": 0.036636082517295526,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.808294677734375,
        "ap": 0.7643592630670912,
        "ap_weighted": 0.7643592630670912,
        "f1": 0.8073180242644954,
        "f1_weighted": 0.8073237852590831,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.808294677734375,
        "scores_per_experiment": [
          {
            "accuracy": 0.803009033203125,
            "ap": 0.7603343403455858,
            "ap_weighted": 0.7603343403455858,
            "f1": 0.8017657975900137,
            "f1_weighted": 0.8017725048341692
          },
          {
            "accuracy": 0.8135986328125,
            "ap": 0.7684260871793961,
            "ap_weighted": 0.7684260871793961,
            "f1": 0.8129141724235112,
            "f1_weighted": 0.8129190071588522
          },
          {
            "accuracy": 0.811431884765625,
            "ap": 0.7669993698103456,
            "ap_weighted": 0.7669993698103456,
            "f1": 0.8106264534950266,
            "f1_weighted": 0.810631730068629
          },
          {
            "accuracy": 0.802459716796875,
            "ap": 0.762437854499481,
            "ap_weighted": 0.762437854499481,
            "f1": 0.8008577157832993,
            "f1_weighted": 0.8008653469551912
          },
          {
            "accuracy": 0.81097412109375,
            "ap": 0.7635986635006484,
            "ap_weighted": 0.7635986635006484,
            "f1": 0.8104259820306259,
            "f1_weighted": 0.8104303372785736
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}