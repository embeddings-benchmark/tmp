{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 176.65593886375427,
  "kg_co2_emissions": 0.0076007054956122935,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.790869140625,
        "ap": 0.7374007833601806,
        "ap_weighted": 0.7374007833601806,
        "f1": 0.7903898786972059,
        "f1_weighted": 0.7903935778850174,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.790869140625,
        "scores_per_experiment": [
          {
            "accuracy": 0.782928466796875,
            "ap": 0.731985297787375,
            "ap_weighted": 0.731985297787375,
            "f1": 0.7821688719508986,
            "f1_weighted": 0.782174367727562
          },
          {
            "accuracy": 0.78839111328125,
            "ap": 0.7282279015831116,
            "ap_weighted": 0.7282279015831116,
            "f1": 0.7883824218668084,
            "f1_weighted": 0.7883830012944378
          },
          {
            "accuracy": 0.79571533203125,
            "ap": 0.7432120212552588,
            "ap_weighted": 0.7432120212552588,
            "f1": 0.7953438860276647,
            "f1_weighted": 0.7953476111308526
          },
          {
            "accuracy": 0.777069091796875,
            "ap": 0.7279715765526572,
            "ap_weighted": 0.7279715765526572,
            "f1": 0.7759057083016314,
            "f1_weighted": 0.7759126068060505
          },
          {
            "accuracy": 0.81024169921875,
            "ap": 0.7556071196224996,
            "ap_weighted": 0.7556071196224996,
            "f1": 0.8101485053390265,
            "f1_weighted": 0.8101503024661838
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}