{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 3808.3969116210938,
  "kg_co2_emissions": 0.08570128507332972,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.80533447265625,
        "ap": 0.7587928258317669,
        "ap_weighted": 0.7587928258317669,
        "f1": 0.8045036908159624,
        "f1_weighted": 0.8045087168609788,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.80533447265625,
        "scores_per_experiment": [
          {
            "accuracy": 0.796600341796875,
            "ap": 0.7536032045261218,
            "ap_weighted": 0.7536032045261218,
            "f1": 0.7951776168547782,
            "f1_weighted": 0.7951849102085641
          },
          {
            "accuracy": 0.802642822265625,
            "ap": 0.7454271370450717,
            "ap_weighted": 0.7454271370450717,
            "f1": 0.8026013429537453,
            "f1_weighted": 0.8026025655018849
          },
          {
            "accuracy": 0.807952880859375,
            "ap": 0.7619188726678389,
            "ap_weighted": 0.7619188726678389,
            "f1": 0.8072180994612819,
            "f1_weighted": 0.8072231844536562
          },
          {
            "accuracy": 0.8128662109375,
            "ap": 0.7733306934698913,
            "ap_weighted": 0.7733306934698913,
            "f1": 0.811590824025497,
            "f1_weighted": 0.8115974469545653
          },
          {
            "accuracy": 0.806610107421875,
            "ap": 0.7596842214499111,
            "ap_weighted": 0.7596842214499111,
            "f1": 0.8059305707845097,
            "f1_weighted": 0.8059354771862235
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}