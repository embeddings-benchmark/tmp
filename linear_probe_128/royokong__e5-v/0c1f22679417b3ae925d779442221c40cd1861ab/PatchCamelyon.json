{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 2079.207501888275,
  "kg_co2_emissions": 0.16987873007276416,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.772833251953125,
        "ap": 0.719057260064476,
        "ap_weighted": 0.719057260064476,
        "f1": 0.7722551493589198,
        "f1_weighted": 0.7722600184290184,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.772833251953125,
        "scores_per_experiment": [
          {
            "accuracy": 0.80120849609375,
            "ap": 0.7485369264499792,
            "ap_weighted": 0.7485369264499792,
            "f1": 0.800922496439701,
            "f1_weighted": 0.8009257202683295
          },
          {
            "accuracy": 0.75421142578125,
            "ap": 0.6991466171458701,
            "ap_weighted": 0.6991466171458701,
            "f1": 0.7535196236107473,
            "f1_weighted": 0.7535252026605094
          },
          {
            "accuracy": 0.769622802734375,
            "ap": 0.7159306740690663,
            "ap_weighted": 0.7159306740690663,
            "f1": 0.7689676178811478,
            "f1_weighted": 0.7689728743785375
          },
          {
            "accuracy": 0.759185791015625,
            "ap": 0.7039166710267397,
            "ap_weighted": 0.7039166710267397,
            "f1": 0.7585935670082176,
            "f1_weighted": 0.7585986755332352
          },
          {
            "accuracy": 0.779937744140625,
            "ap": 0.7277554116307251,
            "ap_weighted": 0.7277554116307251,
            "f1": 0.7792724418547854,
            "f1_weighted": 0.7792776193044807
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}