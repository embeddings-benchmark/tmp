{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 342.1946802139282,
  "kg_co2_emissions": 0.016865725689820373,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.71282958984375,
        "ap": 0.6576322188809577,
        "ap_weighted": 0.6576322188809577,
        "f1": 0.7110280423990171,
        "f1_weighted": 0.7110300055686838,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.71282958984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.743408203125,
            "ap": 0.6940983974005726,
            "ap_weighted": 0.6940983974005726,
            "f1": 0.7412041553577264,
            "f1_weighted": 0.741214359282575
          },
          {
            "accuracy": 0.645477294921875,
            "ap": 0.5902996405892945,
            "ap_weighted": 0.5902996405892945,
            "f1": 0.6421514759050884,
            "f1_weighted": 0.6421367365995375
          },
          {
            "accuracy": 0.72943115234375,
            "ap": 0.6681581785720704,
            "ap_weighted": 0.6681581785720704,
            "f1": 0.729406620709306,
            "f1_weighted": 0.7294077214877746
          },
          {
            "accuracy": 0.720977783203125,
            "ap": 0.6729122937238371,
            "ap_weighted": 0.6729122937238371,
            "f1": 0.7175246176231628,
            "f1_weighted": 0.7175379613488375
          },
          {
            "accuracy": 0.724853515625,
            "ap": 0.6626925841190139,
            "ap_weighted": 0.6626925841190139,
            "f1": 0.7248533423998016,
            "f1_weighted": 0.7248532491246946
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}