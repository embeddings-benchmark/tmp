{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 556.3870816230774,
  "kg_co2_emissions": 0.040634237931382815,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.724017333984375,
        "ap": 0.6634771879037649,
        "ap_weighted": 0.6634771879037649,
        "f1": 0.7228370453328854,
        "f1_weighted": 0.7228358507329352,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.724017333984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.7386474609375,
            "ap": 0.6820324895373034,
            "ap_weighted": 0.6820324895373034,
            "f1": 0.7380537685483127,
            "f1_weighted": 0.7380590965569336
          },
          {
            "accuracy": 0.6820068359375,
            "ap": 0.6177107101299489,
            "ap_weighted": 0.6177107101299489,
            "f1": 0.6776535529743866,
            "f1_weighted": 0.6776375482576104
          },
          {
            "accuracy": 0.736907958984375,
            "ap": 0.6724899615264094,
            "ap_weighted": 0.6724899615264094,
            "f1": 0.7368286059855507,
            "f1_weighted": 0.7368266535391824
          },
          {
            "accuracy": 0.7357177734375,
            "ap": 0.6803901043760631,
            "ap_weighted": 0.6803901043760631,
            "f1": 0.7348431073484496,
            "f1_weighted": 0.7348496138974648
          },
          {
            "accuracy": 0.726806640625,
            "ap": 0.6647626739490999,
            "ap_weighted": 0.6647626739490999,
            "f1": 0.7268061918077268,
            "f1_weighted": 0.7268063414134844
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}