{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 415.1655673980713,
  "kg_co2_emissions": 0.025067854643946234,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.82894287109375,
        "ap": 0.7770753978867504,
        "ap_weighted": 0.7770753978867504,
        "f1": 0.8288151821371474,
        "f1_weighted": 0.8288166012652898,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.82894287109375,
        "scores_per_experiment": [
          {
            "accuracy": 0.82940673828125,
            "ap": 0.7767717741783995,
            "ap_weighted": 0.7767717741783995,
            "f1": 0.8293583139737146,
            "f1_weighted": 0.8293595421264419
          },
          {
            "accuracy": 0.825653076171875,
            "ap": 0.7658664896076984,
            "ap_weighted": 0.7658664896076984,
            "f1": 0.8256209162827086,
            "f1_weighted": 0.8256199045109147
          },
          {
            "accuracy": 0.829620361328125,
            "ap": 0.7802074315788824,
            "ap_weighted": 0.7802074315788824,
            "f1": 0.8294650131353298,
            "f1_weighted": 0.8294672121997374
          },
          {
            "accuracy": 0.830322265625,
            "ap": 0.7846425010461495,
            "ap_weighted": 0.7846425010461495,
            "f1": 0.8299884460501012,
            "f1_weighted": 0.8299916646961402
          },
          {
            "accuracy": 0.8297119140625,
            "ap": 0.7778887930226218,
            "ap_weighted": 0.7778887930226218,
            "f1": 0.8296432212438831,
            "f1_weighted": 0.8296446827932153
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}