{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 154.97462940216064,
  "kg_co2_emissions": 0.006756006158401825,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.728204345703125,
        "ap": 0.6697273683026752,
        "ap_weighted": 0.6697273683026752,
        "f1": 0.7273897548506554,
        "f1_weighted": 0.727393067674686,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.728204345703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.715057373046875,
            "ap": 0.6589405222002941,
            "ap_weighted": 0.6589405222002941,
            "f1": 0.7142765353646141,
            "f1_weighted": 0.7142829169919065
          },
          {
            "accuracy": 0.7279052734375,
            "ap": 0.6599251103495072,
            "ap_weighted": 0.6599251103495072,
            "f1": 0.7268340466801939,
            "f1_weighted": 0.7268267381155729
          },
          {
            "accuracy": 0.736663818359375,
            "ap": 0.6811125221087386,
            "ap_weighted": 0.6811125221087386,
            "f1": 0.7358462383443776,
            "f1_weighted": 0.7358525170718653
          },
          {
            "accuracy": 0.73699951171875,
            "ap": 0.6827161545106035,
            "ap_weighted": 0.6827161545106035,
            "f1": 0.7359065651062189,
            "f1_weighted": 0.7359138237649359
          },
          {
            "accuracy": 0.724395751953125,
            "ap": 0.6659425323442324,
            "ap_weighted": 0.6659425323442324,
            "f1": 0.7240853887578725,
            "f1_weighted": 0.7240893424291497
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}